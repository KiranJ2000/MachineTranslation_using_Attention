{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENG-ITA-Machine-Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AdTZ4C88IsK",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "The goal of this notebook is to convert an **English** sentence to it's **Italian** counterpart. I will implement this task using **Attention**.\n",
        "\n",
        "We could have also used a simple seq-2-seq **encoder-decoder** model , but this model suffers from the infamous **bottleneck problem**, ie, all the information is encoded in to one fixed-length vector.\n",
        "\n",
        "The attention model solves this by allowing the network to **refer back to the input sequence**, instead of forcing it to encode all information into one fixed-length vector.\n",
        "\n",
        "\n",
        "# **Model Architecture**\n",
        "\n",
        "In this notebook , I will be using the **Additive Attention** model by Dzmitry Bahdanau. The model aimed to improve the sequence-to-sequence model in machine translation by **aligning the decoder with the relevant input sentences and implementing Attention.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWAhKKxq7Z_4",
        "colab_type": "code",
        "outputId": "7a0fba87-b9d5-4ba2-a1be-dc593598e2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "import subprocess\n",
        "print(subprocess.getoutput('nvidia-smi'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 31 06:17:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpVSGO7F9hrw",
        "colab_type": "text"
      },
      "source": [
        "Nice! I got **P100 GPU with 12GB memory**. Thanks Google!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqwZCo0A8l4p",
        "colab_type": "code",
        "outputId": "3d61b46d-2457-4e00-dc77-64da77bba325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#Importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import warnings\n",
        "import operator\n",
        "import gzip\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import plotly.graph_objects as go\n",
        "%matplotlib inline\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z391iEUI_I8K",
        "colab_type": "code",
        "outputId": "2465416d-0c3b-4553-f05d-9e12da6c6adb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmiG9BzE_O73",
        "colab_type": "code",
        "outputId": "65aa995a-844d-40af-98f4-1c43d11c8bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#The dataset\n",
        "!wget http://www.manythings.org/anki/ita-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 06:25:21--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.24.108.196, 104.24.109.196, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7345811 (7.0M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   7.00M  10.7MB/s    in 0.7s    \n",
            "\n",
            "2020-05-30 06:25:22 (10.7 MB/s) - ‘ita-eng.zip’ saved [7345811/7345811]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8lzyayw_T_w",
        "colab_type": "code",
        "outputId": "59eadea9-9d4c-49df-c08b-e01993e55865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!unzip ita-eng*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPyaNL8s_YzX",
        "colab_type": "code",
        "outputId": "056ee070-876a-416f-b57b-00cd62bf8679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_about.txt  ita-eng.zip  ita.txt  sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfd0xLqg_Zur",
        "colab_type": "code",
        "outputId": "13f0cc7d-02c4-4a96-c606-bee7ac115c46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "language = pd.read_table('ita.txt',names=['English','Italian','Attribution'])\n",
        "language.head()\n",
        "#data_path = 'ita.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Italian</th>\n",
              "      <th>Attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Ciao!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corri!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Corra!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Correte!</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Chi?</td>\n",
              "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English   Italian                                        Attribution\n",
              "0     Hi.     Ciao!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
              "1    Run!    Corri!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "2    Run!    Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "3    Run!  Correte!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
              "4    Who?      Chi?  CC-BY 2.0 (France) Attribution: tatoeba.org #2..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iomu49q8Bxtn",
        "colab_type": "code",
        "outputId": "bab63b0f-7543-4b76-d662-5070c9dbdf0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "language.drop('Attribution',axis=1,inplace=True)\n",
        "language.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 336614 entries, 0 to 336613\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   English  336614 non-null  object\n",
            " 1   Italian  336614 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 5.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Buvu9GWSB86a",
        "colab_type": "code",
        "outputId": "eafc78e8-18b2-4cb7-a4c4-caffcd349ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('The number of training examples are : {}'.format(language.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of training examples are : 336614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQSwhwXIAPUB",
        "colab_type": "text"
      },
      "source": [
        "The number of training examples is **336614**.Training on the complete dataset  will take a long time. To train faster, we can **limit the size of the dataset to ~80,000 sentences**. We should keep in mind the  **translation quality degrades with less data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0TvCxIQABJ3",
        "colab_type": "code",
        "outputId": "eb9c6858-8d14-4607-d4bd-f93b63b7e174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "value_count = language['English'].map(lambda x : len(x.split())).value_counts()\n",
        "fig = go.FigureWidget(data=[go.Bar(x=value_count.index, y=value_count,\n",
        "                                 marker={'color': value_count,\n",
        "                                               'colorscale':'algae'})]) \n",
        "\n",
        "fig.update_layout(title_text=\"Value Counts of Number of words\",xaxis_title = 'Number of words', yaxis_title = 'Value Count',font=dict(\n",
        "        family=\"Courier New, monospace\",\n",
        "        size=18,\n",
        "        color=\"#7f7f7f\"\n",
        "    ),title_font_size=30)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"cfc738bb-e0d4-4e51-a845-83de50879583\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"cfc738bb-e0d4-4e51-a845-83de50879583\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'cfc738bb-e0d4-4e51-a845-83de50879583',\n",
              "                        [{\"marker\": {\"color\": [73527, 64201, 63081, 43417, 34673, 23288, 11352, 11337, 5453, 2763, 1211, 815, 446, 354, 139, 72, 72, 71, 39, 37, 33, 29, 28, 25, 23, 20, 20, 14, 14, 13, 11, 7, 6, 4, 4, 4, 3, 2, 2, 2, 1, 1], \"colorscale\": [[0.0, \"rgb(214, 249, 207)\"], [0.09090909090909091, \"rgb(186, 228, 174)\"], [0.18181818181818182, \"rgb(156, 209, 143)\"], [0.2727272727272727, \"rgb(124, 191, 115)\"], [0.36363636363636365, \"rgb(85, 174, 91)\"], [0.45454545454545453, \"rgb(37, 157, 81)\"], [0.5454545454545454, \"rgb(7, 138, 78)\"], [0.6363636363636364, \"rgb(13, 117, 71)\"], [0.7272727272727273, \"rgb(23, 95, 61)\"], [0.8181818181818182, \"rgb(25, 75, 49)\"], [0.9090909090909091, \"rgb(23, 55, 35)\"], [1.0, \"rgb(17, 36, 20)\"]]}, \"type\": \"bar\", \"uid\": \"267b4f09-04cc-4f09-a72e-874ffcd42e82\", \"x\": [5, 6, 4, 7, 3, 8, 2, 9, 10, 11, 12, 13, 14, 15, 16, 18, 1, 17, 27, 28, 26, 25, 31, 19, 22, 24, 23, 20, 30, 21, 29, 43, 35, 33, 38, 47, 32, 34, 37, 39, 44, 42], \"y\": [73527, 64201, 63081, 43417, 34673, 23288, 11352, 11337, 5453, 2763, 1211, 815, 446, 354, 139, 72, 72, 71, 39, 37, 33, 29, 28, 25, 23, 20, 20, 14, 14, 13, 11, 7, 6, 4, 4, 4, 3, 2, 2, 2, 1, 1]}],\n",
              "                        {\"font\": {\"color\": \"#7f7f7f\", \"family\": \"Courier New, monospace\", \"size\": 18}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"size\": 30}, \"text\": \"Value Counts of Number of words\"}, \"xaxis\": {\"title\": {\"text\": \"Number of words\"}}, \"yaxis\": {\"title\": {\"text\": \"Value Count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cfc738bb-e0d4-4e51-a845-83de50879583');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4jqJbKCDE_1",
        "colab_type": "text"
      },
      "source": [
        "It is clear from the above **bar-plot** that the distribution of the data is **right-skewed.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz49AJnH9Ui-",
        "colab_type": "code",
        "outputId": "27507334-f6b1-4907-ce4a-eeae45223865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "language = language[language['English'].map(lambda x: len(x.split()))<18]\n",
        "language.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 336200 entries, 0 to 336425\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count   Dtype \n",
            "---  ------   --------------   ----- \n",
            " 0   English  336200 non-null  object\n",
            " 1   Italian  336200 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 7.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flQvcfaQChrx",
        "colab_type": "code",
        "outputId": "87ba6d87-ae6a-43df-bac7-d73ed61a2e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#Shuffling and decreasing the dataset\n",
        "language = shuffle(language)\n",
        "language = language[:70000]\n",
        "language.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Italian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>166870</th>\n",
              "      <td>Tom is a freelance writer.</td>\n",
              "      <td>Tom è uno scrittore freelance.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149708</th>\n",
              "      <td>Stand back from the rope.</td>\n",
              "      <td>Stai lontana dalla corda.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281191</th>\n",
              "      <td>He has a good head on his shoulders.</td>\n",
              "      <td>Ha una buona testa sulle spalle.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58956</th>\n",
              "      <td>I need to call Tom.</td>\n",
              "      <td>Io devo chiamare Tom.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78693</th>\n",
              "      <td>Tom started working.</td>\n",
              "      <td>Tom iniziò a lavorare.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     English                           Italian\n",
              "166870            Tom is a freelance writer.    Tom è uno scrittore freelance.\n",
              "149708             Stand back from the rope.         Stai lontana dalla corda.\n",
              "281191  He has a good head on his shoulders.  Ha una buona testa sulle spalle.\n",
              "58956                    I need to call Tom.             Io devo chiamare Tom.\n",
              "78693                   Tom started working.            Tom iniziò a lavorare."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spqu-NI8Qfii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the dataset\n",
        "with open('language_training_data.pickle','wb') as handle:\n",
        "  pickle.dump(language,handle,protocol = pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OapyhNRCRNZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the dataset\n",
        "with open('language_training_data.pickle','rb') as handle:\n",
        "  language_nmt = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nP8OeR9Rpit",
        "colab_type": "code",
        "outputId": "0e074c2b-1ac1-443c-d943-8a43a1181f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "language_nmt.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Italian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>166870</th>\n",
              "      <td>Tom is a freelance writer.</td>\n",
              "      <td>Tom è uno scrittore freelance.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149708</th>\n",
              "      <td>Stand back from the rope.</td>\n",
              "      <td>Stai lontana dalla corda.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281191</th>\n",
              "      <td>He has a good head on his shoulders.</td>\n",
              "      <td>Ha una buona testa sulle spalle.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58956</th>\n",
              "      <td>I need to call Tom.</td>\n",
              "      <td>Io devo chiamare Tom.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78693</th>\n",
              "      <td>Tom started working.</td>\n",
              "      <td>Tom iniziò a lavorare.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     English                           Italian\n",
              "166870            Tom is a freelance writer.    Tom è uno scrittore freelance.\n",
              "149708             Stand back from the rope.         Stai lontana dalla corda.\n",
              "281191  He has a good head on his shoulders.  Ha una buona testa sulle spalle.\n",
              "58956                    I need to call Tom.             Io devo chiamare Tom.\n",
              "78693                   Tom started working.            Tom iniziò a lavorare."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A4K9XG_qWTb",
        "colab_type": "code",
        "outputId": "bf7a72b9-9b34-48b1-d870-69acd90f07a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "language_nmt.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 70000 entries, 166870 to 179364\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   English  70000 non-null  object\n",
            " 1   Italian  70000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTirJEAhE_Z8",
        "colab_type": "text"
      },
      "source": [
        "# **Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zAb69Z1D-j0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  text = re.sub('\\,','',text)\n",
        "  text = re.sub(r'[!?$@#*]','',text)\n",
        "  text = re.sub('\\.',' ',text)\n",
        "  text = '<start> ' + text + ' <end>'\n",
        "  return text\n",
        "\n",
        "def clean_eng_words(text):\n",
        "  text = re.sub(\"aren't\", \"are not\",text)\n",
        "  text = re.sub(\"can't\",\"cannot\",text)\n",
        "  text = re.sub(\"don't\",\"do not\",text)\n",
        "  text = re.sub(\"couldn't\",\"could not\",text)\n",
        "  text = re.sub(\"doesn't\",\"does not\",text)\n",
        "  text = re.sub(\"hadn't\",\"had not\",text)\n",
        "  text = re.sub(\"where's\",\"where is\",text)\n",
        "  text = re.sub(\"wouldn't\",\"would not\",text)\n",
        "  text = re.sub(\"he'll\",\"he will\",text)\n",
        "  text = re.sub(\"what've\",\"what have\",text)\n",
        "  text = re.sub(\"who'd\",\"who would\",text)\n",
        "  text = re.sub(\"haven't\",\"have not\",text)\n",
        "  text = re.sub(\"who'll\",\"who will\",text)\n",
        "  text = re.sub(\"i'll\",\"i will\",text)\n",
        "  text = re.sub(\"i've\",\"i have\",text)\n",
        "  text = re.sub(\"i'm\",\"i am\",text)\n",
        "  text = re.sub(\"we're\",\"we are\",text)\n",
        "  text = re.sub(\"he's\",\"he is\",text)\n",
        "  text = re.sub(\"i'd\",\"i would\",text)\n",
        "  text = re.sub(\"you'd\",\"you would\",text)\n",
        "  text = re.sub(\"you'll\",\"you will\",text)\n",
        "  text = re.sub(\"you're\",\"you are\",text)\n",
        "  text = re.sub(\"you've\",\"you have\",text)\n",
        "  text = re.sub(\"wasn't\",\"was not\",text)\n",
        "  text = re.sub(\"that's\",\"that is\",text)\n",
        "  text = re.sub(\"isn't\",\"is not\",text)\n",
        "  text = re.sub(\"didn't\",\"did not\",text)\n",
        "  text = re.sub(\"they've\",\"they have\",text)\n",
        "  text = re.sub(\"they're\",\"they are\",text)\n",
        "  text = re.sub(\"they'll\",\"they will\",text)\n",
        "  text = re.sub(\"what's\",\"what is\",text)\n",
        "  text = re.sub(\"what're\",\"what are\",text)\n",
        "  text = re.sub(\"what'll\",\"what will\",text)\n",
        "  text = re.sub(\"there's\",\"there is\",text)\n",
        "  text = re.sub(\"it's\",\"it is\",text)\n",
        "  text = re.sub(\"it'll\",\"it will\",text)\n",
        "  text = re.sub(\"could've\",\"could have\",text)\n",
        "  text = re.sub(\"it'll\",\"it will\",text)\n",
        "  text = re.sub(\"shouldn't\",\"should not\",text)\n",
        "  text = re.sub(\"should've\",\"should have\",text)\n",
        "  text = re.sub(\"shan't\",\"shall not\",text)\n",
        "  text = re.sub(\"won't\",\"will not\",text)\n",
        "  text = re.sub(\"we'd\",\"we would\",text)\n",
        "  text = re.sub(\"let's\",\"let us\",text)\n",
        "  text = re.sub(\"that'll\",\"that will\",text)\n",
        "  text = re.sub(\"weren't\",\"were not\",text)\n",
        "  return text\n",
        "\n",
        "language_nmt['English'] = language_nmt['English'].apply(lambda x: preprocess(x))\n",
        "language_nmt['Italian'] = language_nmt['Italian'].apply(lambda x: preprocess(x))\n",
        "language_nmt['English'] = language_nmt['English'].apply(lambda x: clean_eng_words(x))\n",
        "language_nmt.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzz9xGpUEUG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the embedding matrix\n",
        "ps = PorterStemmer() \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_embedding(filename,word_index,vocab_len,dim):\n",
        "  embedding_index={}\n",
        "  if filename.split('.')[-1] == 'vec':\n",
        "    f = open(filename,encoding='utf-8')\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coeff = np.array(values[1:],dtype='float32')\n",
        "      embedding_index[word] = coeff\n",
        "    f.close()\n",
        "  elif filename.split('.')[-1] == 'gz':\n",
        "    f = gzip.open(filename)\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0].decode(encoding='utf-8')\n",
        "      coeff = np.asarray(values[1:],dtype='float32')\n",
        "      embedding_index[word] = coeff\n",
        "    f.close()\n",
        "  embedding_matrix = np.zeros((vocab_len+1,dim))\n",
        "  for word,index in tqdm(word_index.items()):\n",
        "    if index>vocab_len:\n",
        "      continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if word == '<oov>':\n",
        "      embedding_vector = np.random.randn(300,)*0.02\n",
        "    if embedding_vector is None:\n",
        "      embedding_vector = embedding_index.get(lemmatizer.lemmatize(word))\n",
        "    elif embedding_vector is None:\n",
        "      embedding_vector = embedding_index.get(ps.stem(word))\n",
        "    elif embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector\n",
        "    \n",
        "  return embedding_matrix,embedding_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0k-LodPWmEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenizing and padding the input and output corpus\n",
        "def tokenize(corpus):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<oov>')\n",
        "  lang_tokenizer.fit_on_texts(corpus)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(corpus)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "\n",
        "  return tensor,lang_tokenizer\n",
        "\n",
        "input_tensor , input_tokenizer = tokenize(np.asarray(language_nmt['English']))\n",
        "output_tensor , output_tokenizer = tokenize(np.asarray(language_nmt['Italian']))\n",
        "vocab_inp_size = len(input_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(output_tokenizer.word_index)+1\n",
        "max_length_targ, max_length_inp = output_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxrc-7TBY7cs",
        "colab_type": "code",
        "outputId": "4731ed9b-f85d-4872-c40b-ad740a0bcc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('The maximum length of the English corpus is : {}'.format(max_length_inp))\n",
        "print('The maximum length of the Italian corpus is : {}'.format(max_length_targ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The maximum length of the English corpus is : 21\n",
            "The maximum length of the Italian corpus is : 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KtVQ6M-aClN",
        "colab_type": "code",
        "outputId": "6713ca3e-a015-4666-8be2-40a628409ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "# Importing the pre-trained embeddings for english. I will be using the 300 dimensional fast text embeddings\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-31 06:20:15--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v  10%[=>                  ]  65.72M  18.2MB/s    in 3.6s    \n",
            "\n",
            "2020-05-31 06:20:19 (18.2 MB/s) - Read error at byte 68911547/681808098 (Connection reset by peer). Retrying.\n",
            "\n",
            "--2020-05-31 06:20:20--  (try: 2)  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 681808098 (650M), 612896551 (585M) remaining [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[++=================>] 650.22M  20.8MB/s    in 29s     \n",
            "\n",
            "2020-05-31 06:20:49 (20.3 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt-ksVthpKI5",
        "colab_type": "code",
        "outputId": "fd298487-9e54-4696-9802-97ac29e08840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!unzip wiki-news-300d-1M.vec*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkQ9HBCQplKm",
        "colab_type": "code",
        "outputId": "7cfbbb56-8627-42a8-cbfe-198374a771d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language_training_data.pickle  wiki-news-300d-1M.vec\n",
            "sample_data\t\t       wiki-news-300d-1M.vec.zip\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZWw9pH2ppFe",
        "colab_type": "code",
        "outputId": "868a79e5-0139-4dea-e326-49e0d4a53a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embedding_matrix_eng,embedding_index_eng = get_embedding('wiki-news-300d-1M.vec',input_tokenizer.word_index,vocab_inp_size,300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9127/9127 [00:01<00:00, 4678.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QpacOqUppr5",
        "colab_type": "code",
        "outputId": "f68e3a9b-dcef-4690-db0f-579de4c7d5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#Importing the pre-trained embeddings for italian\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.vec.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-31 06:22:37--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.it.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4b8e, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1272825284 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.it.300.vec.gz’\n",
            "\n",
            "cc.it.300.vec.gz    100%[===================>]   1.18G  22.3MB/s    in 55s     \n",
            "\n",
            "2020-05-31 06:23:33 (22.0 MB/s) - ‘cc.it.300.vec.gz’ saved [1272825284/1272825284]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm25rUTypqjR",
        "colab_type": "code",
        "outputId": "b82fba91-a3a6-4389-9382-cbfe6db447a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "embedding_matrix_ita,embedding_index_ita = get_embedding('cc.it.300.vec.gz',output_tokenizer.word_index,vocab_tar_size,300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16370/16370 [00:00<00:00, 302136.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L3IlXzArCtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking the coverage of english and italian words by the embeddings\n",
        "def check_coverage(vocab, embeddings_index):\n",
        "\n",
        "  known_words = {}\n",
        "  unknown_words = {}\n",
        "  nb_known_words = 0\n",
        "  nb_unknown_words = 0\n",
        "  for word in vocab.keys():\n",
        "    try:\n",
        "        known_words[word] = embeddings_index[word]\n",
        "        nb_known_words += vocab[word]\n",
        "    except:\n",
        "        unknown_words[word] = vocab[word]\n",
        "        nb_unknown_words += vocab[word]\n",
        "        pass\n",
        "  print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n",
        "  print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
        "  unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "  return unknown_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfz6WA0ErnH6",
        "colab_type": "code",
        "outputId": "a0659c18-6c06-4cc6-e88a-976a356a76b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "eng_oov = check_coverage(input_tokenizer.word_index,embedding_index_eng)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 96.494% of vocab\n",
            "Found embeddings for  95.550% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PhjYtX4rnmS",
        "colab_type": "code",
        "outputId": "1423c147-6fb6-470c-f8cb-82afbec4ef1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "eng_oov[:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"when's\", 9103),\n",
              " ('\"king', 9092),\n",
              " (\"miller's\", 9090),\n",
              " ('10:00', 9027),\n",
              " ('\"tom', 9014),\n",
              " (\"lidya's\", 9011),\n",
              " (\"water's\", 9007),\n",
              " ('naka-meguro', 9004),\n",
              " (\"plan's\", 8996),\n",
              " ('30-passenger', 8986),\n",
              " (\"kunio's\", 8937),\n",
              " (\"else's\", 8925),\n",
              " ('guzmán', 8893),\n",
              " ('\"neither', 8863),\n",
              " (\"horse's\", 8827)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT_MuitquOtM",
        "colab_type": "text"
      },
      "source": [
        "Nice! We found **96%** embeddings for the English vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz9HDzyhrn2J",
        "colab_type": "code",
        "outputId": "bb330e8a-4a67-49bd-9ce0-6a35f472995b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "oov_ita = check_coverage(output_tokenizer.word_index,embedding_index_ita)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 92.883% of vocab\n",
            "Found embeddings for  91.428% of all text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27p3lzQVuWhL",
        "colab_type": "text"
      },
      "source": [
        "In the case of the target language, we found **93%** embeddings in the vocabulary, which is pretty good.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHuN4hyGY9Fa",
        "colab_type": "code",
        "outputId": "4857117a-b537-4551-f69c-554b50b8c8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Creating training and validation splits\n",
        "input_tensor_train,input_tensor_val,output_tensor_train,output_tensor_val = train_test_split(input_tensor,output_tensor,test_size=0.1)\n",
        "\n",
        "print('Length of Training set : {}'.format(len(input_tensor_train)))\n",
        "print('Length of Validation set : {}'.format(len(input_tensor_val)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Training set : 63000\n",
            "Length of Validation set : 7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iV7PQnN7DGf",
        "colab_type": "text"
      },
      "source": [
        "We will use **tf.data.Dataset.from_tensor_slices()** method to get slices of the array in the form of an object, since the dataset is big and we want to create the dataset in memory to be efficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qEKjAXSq04l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512\n",
        "units = 1024\n",
        "embedding_dim = 300\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,output_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE,drop_remainder = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLRqP_fu-6pq",
        "colab_type": "text"
      },
      "source": [
        "# **Implementing the Encoder and Decoder model with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM2SpZ6Lq2C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,batches,vocab_size,embedding_dim,enc_units,embedding_matrix):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.batches = batches\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size+1,embedding_dim,weights=[embedding_matrix],trainable=True)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  def call(self,x,hidden):\n",
        "    x = self.embedding(x)\n",
        "    output,state = self.gru(x,initial_state=hidden)\n",
        "    output = self.dropout(output)\n",
        "    return output,state\n",
        "\n",
        "  def initialize_hidden_states(self):\n",
        "    return tf.zeros((self.batches,self.enc_units))\n",
        "    #init_state = [np.zeros((self.batches, self.enc_units)) for i in range(2)]\n",
        "    #return init_state\n",
        "\n",
        "class GlobalBahdanauAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,units):\n",
        "    super(GlobalBahdanauAttention,self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self,hidden_state,encoder_output):\n",
        "    #We need to expand the dimension of hidden_state from (batch_size,hidden_size) to (batch_size,1,hidden_size)\n",
        "    hidden_state_with_time_axis = tf.expand_dims(hidden_state,axis=1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(hidden_state_with_time_axis) + self.W2(encoder_output)))\n",
        "    attention_weights = tf.nn.softmax(score,axis=1)\n",
        "    #Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, \n",
        "    # since the shape of score is (batch_size, max_length, hidden_size).\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    context_vector = tf.reduce_sum(context_vector,axis=1)\n",
        "    return context_vector \n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,batches,vocab_size,embedding_dim,dec_units,embedding_matrix):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.batches = batches\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size+1,embedding_dim,weights=[embedding_matrix],trainable=True)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = GlobalBahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self,x,hidden,enc_output):\n",
        "    context_vector  = self.attention(hidden,enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector,axis=1),x],axis=-1)\n",
        "    output,state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.dense(output)\n",
        "    return x,state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz7JJ7d3H11W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(BATCH_SIZE,vocab_inp_size,embedding_dim,units,embedding_matrix_eng)\n",
        "decoder = Decoder(BATCH_SIZE,vocab_tar_size,embedding_dim,units,embedding_matrix_ita)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQIF3QoRsGdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining the optimizer and loss functions\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "#optimizer = tf.keras.optimizers.RMSprop()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "\n",
        "def loss_fn(real,pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "  loss_ = loss(real,pred)\n",
        "  mask = tf.cast(mask,dtype = loss_.dtype)\n",
        "  loss_*= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h0_YJpSIyuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'checkpoints_training'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"training_nmt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A-c59YsbA5m",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AVNy57lUHO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def training(inputs,target,enc_hidden):\n",
        "  loss=0\n",
        "  #Creating a custom training with gradient tape\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    encoder_output,encoder_hidden = encoder(inputs,enc_hidden)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    for t in range(1,target.shape[1]):\n",
        "      pred,decoder_hidden = decoder(dec_input,decoder_hidden,encoder_output)\n",
        "      loss += loss_fn(target[:, t], pred)\n",
        "      #Using teacher forcing during training\n",
        "      dec_input = tf.expand_dims(target[:, t], 1)\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "  trainable_var = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss,trainable_var)\n",
        "  optimizer.apply_gradients(zip(gradients,trainable_var))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSOq4KfAdLgM",
        "colab_type": "code",
        "outputId": "d470a34b-456c-41f1-f4f4-d8b0364306d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epochs in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_states()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch,(inp,targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = training(inp,targ,enc_hidden)\n",
        "    total_loss+= batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epochs + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  if (epochs + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Epoch {} Loss {:.4f}'.format(epochs + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.6615\n",
            "Epoch 1 Batch 100 Loss 1.5855\n",
            "Epoch 1 Loss 1.6797\n",
            "Time taken for 1 epoch 102.98138952255249 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4861\n",
            "Epoch 2 Batch 100 Loss 1.1891\n",
            "Epoch 2 Loss 1.3061\n",
            "Time taken for 1 epoch 77.84406971931458 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1668\n",
            "Epoch 3 Batch 100 Loss 0.9754\n",
            "Epoch 3 Loss 1.0461\n",
            "Time taken for 1 epoch 76.56199908256531 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9290\n",
            "Epoch 4 Batch 100 Loss 0.7826\n",
            "Epoch 4 Loss 0.8411\n",
            "Time taken for 1 epoch 77.97594285011292 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6980\n",
            "Epoch 5 Batch 100 Loss 0.6026\n",
            "Epoch 5 Loss 0.6493\n",
            "Time taken for 1 epoch 76.4459331035614 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5515\n",
            "Epoch 6 Batch 100 Loss 0.4706\n",
            "Epoch 6 Loss 0.4896\n",
            "Time taken for 1 epoch 77.75262546539307 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3960\n",
            "Epoch 7 Batch 100 Loss 0.3685\n",
            "Epoch 7 Loss 0.3731\n",
            "Time taken for 1 epoch 76.44320464134216 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2780\n",
            "Epoch 8 Batch 100 Loss 0.2981\n",
            "Epoch 8 Loss 0.2874\n",
            "Time taken for 1 epoch 77.97632813453674 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2362\n",
            "Epoch 9 Batch 100 Loss 0.2401\n",
            "Epoch 9 Loss 0.2299\n",
            "Time taken for 1 epoch 76.48134350776672 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1878\n",
            "Epoch 10 Batch 100 Loss 0.1939\n",
            "Epoch 10 Loss 0.1881\n",
            "Time taken for 1 epoch 77.87049579620361 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.1558\n",
            "Epoch 11 Batch 100 Loss 0.1563\n",
            "Epoch 11 Loss 0.1596\n",
            "Time taken for 1 epoch 76.26047539710999 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1318\n",
            "Epoch 12 Batch 100 Loss 0.1457\n",
            "Epoch 12 Loss 0.1359\n",
            "Time taken for 1 epoch 78.02125382423401 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1084\n",
            "Epoch 13 Batch 100 Loss 0.1197\n",
            "Epoch 13 Loss 0.1182\n",
            "Time taken for 1 epoch 76.36427402496338 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0891\n",
            "Epoch 14 Batch 100 Loss 0.1128\n",
            "Epoch 14 Loss 0.1039\n",
            "Time taken for 1 epoch 77.95948696136475 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0782\n",
            "Epoch 15 Batch 100 Loss 0.0905\n",
            "Epoch 15 Loss 0.0928\n",
            "Time taken for 1 epoch 76.46104216575623 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0766\n",
            "Epoch 16 Batch 100 Loss 0.0818\n",
            "Epoch 16 Loss 0.0833\n",
            "Time taken for 1 epoch 77.80947875976562 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0610\n",
            "Epoch 17 Batch 100 Loss 0.0795\n",
            "Epoch 17 Loss 0.0751\n",
            "Time taken for 1 epoch 76.32147073745728 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0638\n",
            "Epoch 18 Batch 100 Loss 0.0674\n",
            "Epoch 18 Loss 0.0689\n",
            "Time taken for 1 epoch 77.93509888648987 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0550\n",
            "Epoch 19 Batch 100 Loss 0.0691\n",
            "Epoch 19 Loss 0.0645\n",
            "Time taken for 1 epoch 76.29780316352844 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0543\n",
            "Epoch 20 Batch 100 Loss 0.0632\n",
            "Epoch 20 Loss 0.0614\n",
            "Time taken for 1 epoch 77.82108068466187 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0482\n",
            "Epoch 21 Batch 100 Loss 0.0626\n",
            "Epoch 21 Loss 0.0571\n",
            "Time taken for 1 epoch 76.47666358947754 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0447\n",
            "Epoch 22 Batch 100 Loss 0.0595\n",
            "Epoch 22 Loss 0.0554\n",
            "Time taken for 1 epoch 77.82296514511108 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0464\n",
            "Epoch 23 Batch 100 Loss 0.0608\n",
            "Epoch 23 Loss 0.0566\n",
            "Time taken for 1 epoch 76.48217797279358 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0479\n",
            "Epoch 24 Batch 100 Loss 0.0484\n",
            "Epoch 24 Loss 0.0523\n",
            "Time taken for 1 epoch 77.6818151473999 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0416\n",
            "Epoch 25 Batch 100 Loss 0.0532\n",
            "Epoch 25 Loss 0.0489\n",
            "Time taken for 1 epoch 76.51415777206421 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0408\n",
            "Epoch 26 Batch 100 Loss 0.0512\n",
            "Epoch 26 Loss 0.0466\n",
            "Time taken for 1 epoch 77.76788330078125 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0345\n",
            "Epoch 27 Batch 100 Loss 0.0493\n",
            "Epoch 27 Loss 0.0444\n",
            "Time taken for 1 epoch 76.15536618232727 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0357\n",
            "Epoch 28 Batch 100 Loss 0.0465\n",
            "Epoch 28 Loss 0.0430\n",
            "Time taken for 1 epoch 77.8503770828247 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0347\n",
            "Epoch 29 Batch 100 Loss 0.0457\n",
            "Epoch 29 Loss 0.0425\n",
            "Time taken for 1 epoch 76.28571844100952 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0390\n",
            "Epoch 30 Batch 100 Loss 0.0499\n",
            "Epoch 30 Loss 0.0444\n",
            "Time taken for 1 epoch 77.65689015388489 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxFzMu4Af18_",
        "colab_type": "text"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mgl_7LkeWiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "  sentence = clean_eng_words(sentence)\n",
        "  #inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = input_tokenizer.texts_to_sequences([sentence])\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen = max_length_inp,padding='post')                                                                                                             \n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "\n",
        "    predictions, dec_hidden = decoder(dec_input,dec_hidden,enc_out)                                                                                          \n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    result += output_tokenizer.index_word[predicted_id] + ' '\n",
        "    if output_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne5dA8QlkvJO",
        "colab_type": "code",
        "outputId": "082ec802-217a-4cae-8db6-746194887752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "!pip install googletrans"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2020.4.5.1)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15777 sha256=50fc794dbd190bbc7e5062a805a5f0df82a4126d592d57c885e02e7af647de34\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/e7/a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NOF0AzGk9i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence= evaluate(sentence)\n",
        "\n",
        "  print('Input                           : %s' % (sentence[7:-5]))\n",
        "  print('\\n')\n",
        "  print('Predicted translation           : {}'.format(result[:-6]))\n",
        "  print('\\n')\n",
        "  print('Predicted translation in English: {}'.format(translator.translate(result[:-6]).text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uo3Mcj0JTtM",
        "colab_type": "code",
        "outputId": "38879b48-37ed-4458-8037-f0f7a99ecbbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe130307a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIvSb2-Vfc7e",
        "colab_type": "text"
      },
      "source": [
        "**Let's test the model on samples it has never seen before.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKp_4o3olDWE",
        "colab_type": "code",
        "outputId": "c1dbd2ef-b860-4d64-e95c-ad5afe4c8392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('i am very happy today')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i am very happy today \n",
            "\n",
            "\n",
            "Predicted translation           : io sono molto felice oggi \n",
            "\n",
            "\n",
            "Predicted translation in English: I am very happy today\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZpjvmNPrY19",
        "colab_type": "code",
        "outputId": "4263230f-ed12-4630-d1fe-73c3015ca8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate(\"i don't know what i will do with my life\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i do not know what i will do with my life \n",
            "\n",
            "\n",
            "Predicted translation           : io non so cosa voglio con la mia vita \n",
            "\n",
            "\n",
            "Predicted translation in English: I do not know what I want with my life\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL-oJN3j53Ml",
        "colab_type": "code",
        "outputId": "a4f19362-819a-4839-9973-016bacf7c783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('i am not going to do anything today')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i am not going to do anything today \n",
            "\n",
            "\n",
            "Predicted translation           : io non faccio niente oggi \n",
            "\n",
            "\n",
            "Predicted translation in English: I do not do anything today\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STUDCtY7ZbAZ",
        "colab_type": "code",
        "outputId": "39bc7ed6-2274-453f-9a75-53dbbc35565e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('the moon is very big today')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  the moon is very big today \n",
            "\n",
            "\n",
            "Predicted translation           : la luna è molto grande oggi \n",
            "\n",
            "\n",
            "Predicted translation in English: the moon is very large today\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pksy4uhZZ8YP",
        "colab_type": "code",
        "outputId": "1ee2391a-cb00-49ae-9184-6cb3a8d0e48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('i hate you')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i hate you \n",
            "\n",
            "\n",
            "Predicted translation           : ti odio \n",
            "\n",
            "\n",
            "Predicted translation in English: I hate you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT1Wi93VmRS0",
        "colab_type": "code",
        "outputId": "f80b752b-eaa8-4d79-f9b2-0422142955f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('i really want a dog')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i really want a dog \n",
            "\n",
            "\n",
            "Predicted translation           : io voglio davvero un cane \n",
            "\n",
            "\n",
            "Predicted translation in English: I really want a dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdnGt7vAm-wQ",
        "colab_type": "code",
        "outputId": "9eca764c-4d86-44cf-f2d9-b5fee7e4dfff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('i am not feeling well because i ate a lot of food yesterday')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  i am not feeling well because i ate a lot of food yesterday \n",
            "\n",
            "\n",
            "Predicted translation           : non mi sto sentendo bene perché ho mangiato così cibo ieri \n",
            "\n",
            "\n",
            "Predicted translation in English: I'm not feeling well because I ate so food yesterday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1tkXnHwnrQT",
        "colab_type": "code",
        "outputId": "713006f1-650c-45ef-965b-6c4a0e840955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "translate('she likes to sing but not dance')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input                           :  she likes to sing but not dance \n",
            "\n",
            "\n",
            "Predicted translation           : a lei piace cantare però non danza \n",
            "\n",
            "\n",
            "Predicted translation in English: she likes to sing but not dance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYmvQQBbBbu",
        "colab_type": "text"
      },
      "source": [
        "### Wow! Even though the model was trained with just **70000 samples and only trained for 30 epochs, the model is giving very good results on samples which it has not seen!.** This shows the power of Attention! The Attention mechanism has revolutionised the way we create NLP models and is currently a standard fixture in most state-of-the-art NLP models. This is because it enables the model to “remember” all the words in the input and focus on specific words when formulating a response.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Even though the model gave good results on samples which it has   never seen  before, some were not perfect. The challenge of training an effective model can be attributed largely to the lack of training data and training time. Due to the complex nature of the different languages involved and a large number of vocabulary and grammatical permutations, an effective model will require tons of data and training time before any perfect results can be seen on evaluation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EWGDk9w03rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}